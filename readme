本系列为nlp相关原理和实践，语言为python
1、中文分词技术
  参考word_segmentation.py文件，包含基于规则分词分词方法，统计分词方法，以及py示例代码。
  结巴分词示例
  1 规则分词
   1）正向最大分词
        基本思想是：假定分词词典中最长词有一个汉字字符，则用被处理文档的当前字串的前i个字作为匹配字段，查找
        字典，若字典中存在这样的一个长度为i的字词，则匹配成功，匹配字段被做为一个词切分出来。如果字典中
        没有找到这个词，则匹配失败，将词的最后一个字去掉，对剩下的字串重新进行匹配处理。
        如此进行，知道匹配成功，即切出一个词。

   2）逆向最大匹配
        思想与正向相反。
   3）双向最大匹配
        思想：将正向与反向最大分词的结果进行比较，按照最大匹配原则，选取词数切分最少的最为结果。

  2、统计分词
  主要思想是：
  把每个词看作是由词的最小单位的各个字组成的，如果相连的字在不同的文本中出现次数越多，就证明这相连的字就很可能就是一个词。
  因此我们就可以利用字与字相邻出现的频率来反应词的可靠性，统计语料中相邻共现的各个字组合的频率，当这个频率高于某个临界值，
  就可以组成一个词。
  基于统计的分词：
  1）建立统计语言模型
  2）对句子进行单词划分，然后对分词结果进行概率计算，获得概率最大的分词结果。比如：HMM 和CRF

  语言模型：
  用概率论的专业术解释语言模型就是：为长度为m的字符串确定其概率分布。p(w1,w2.....wm),其中w1...wm依次表示文本中的各个词语。一般用链式法则计
  算概率值 p(w1,w2....)= P(w1)P(w2|w1)P(w3|w1,w2)....P(wm|w1,w2....wm-1)
  上式当文本过长，计算难度越来越大。
  n——gram模型：
  n=1:p(w1,w2....)=p(w1)p(w2)...p(wm)
  n=2:p(w1,w2....)=p(w1)p(w2|pw1)...p(wi|wi-1)
  n=2:p(w1,w2....)=p(w1)p(w2|pw1)...p(wi|wi-1,wi-2)

  HMM模型：
  隐马尔科夫模型将分词作为字在字符串中序列标注任务来实现的。基本思路是：每个字在构造一个特定的词语时都占据着一个确定的构词位置，规定每个字最多有
  四个构词位置：B词首，M词中，E词尾，S单独成词。
  用数学抽象表示：
  X=X1X2...Xn代表输入的句子，n表示句子的长度，Xn表示句中的字，Y=Y1Y2...Yn，为BMSE四中标记
  这里引入观测独立性行假设：即每个字段输出仅仅与当前字有关
  于是能得到公式：p(y1y2...yn|x1x2...xn) = p(y1|x1).....p(yn|xn)
  基于观测独立性假设，完全没有考虑上下文，计算容易，但是会出现不合理情况。
  引入HMM解决这个问题。
  上式中解决p（y|x）问题，根据贝叶斯公式，p（Y|X）= p(x|y)p(y)/p（x）
  对p（x|y）做马尔科夫驾驶
  p（x|y）=p(x1|y1)p(x2|y2)...p(xn|yn)
  对p(y)引入马尔科夫另一假设，每个输出仅仅个上一个输出有关系。
  p（y）= p(y1)p(y2|y1)...p(yn|yn-1)
  求解HMM的过程中使用了vetervi算法：核心思想是如果最终的最优路径经过某个xi，那么从最初节点到xi-1节点的路径也是最优路径。
2、词性标注和命名实体识别
  原理知识
  CRF的命名实体识别
  地名识别
  CRF：设X =(X1,X2...Xn) 和Y=(Y1,Y2...Yn)是联合随机变量，若随机变量y构成一个无向图表示的马尔科夫，则其条件概率P(Y|X)称为条件随机场。
  CRF++实现中文
3、关键词提取算法
  1）TF/IDF是一种基于统计的计算方法，常用于估计在一个文档集中一个词对某份文档的重要程度。包括两个算法：TF和IDF部分。TF算法是统计一个词在一篇
  文档中出现的频次，其基本思想是一个词在一篇文档中出现的次数越多，则其对文档的表达能力也越强。而IDF算法统计一个词在文档集的多少个文档中出现。
  其基本思想是一个词在越少的文档中出现，则其对文档的区分能力也就越强。
  tf = 文档中出现的次数n/文档总次数
  idf = log（总文档数/(1+Di)） 其中Di为文档集中出现过词i的文档数量。
  tf—idf算法为： tf*idf
  2）TextRank算法
  与其他算法不同的是，TextRank算法可以脱离语料库的背景，仅仅对单篇文档进行分析就可以提取该文档的关键词。而TF—IDF算法需要统计每个词在语料库中的
  多少个文档中出现过，也就是逆文档评率，主题模型的关键词提取算法则是通过大规模文档的学习，来发现文档的隐含主题。
  TextRank 算法来源于pageRank算法
  PageRank算法的基本思想是：
    连接数量。一个网页被越多的其他网页连接，说明这个网页越重要。
    连接质量。一个网页被越高权值的网页连接，说明这个越重要。
    [1/out(Vj)]*S(Vj) 即为Vj贡献给vi的分数。将Vi的所有入链的贡献值加起来就是该网页的得分。
    S(Vi)=∑{...}
    其中out（Vj）表示表示j出链的数量，S(Vj)表示一个网页的PR值，即PageRank值。
    上式会导致一些孤立的网页不会被访问到，为了便面这种情况，我们队计算公式加入了一个阻尼系数d
    即：S(Vi) = (1-d)+d*∑{...}
  与PageRank有向无权图不同的是，textrank是 有权图
    即：WS(Vi)=(1-d)+d*∑{W*...}
  在TextRank进行关键词提取时候，使用一个窗口的概念，在窗口中的词之间都有连接关系。比如：
    世界献血日，学校团体，献血服务志愿者等可到血液中心参观....
    进过分词后[世界 献血，日，学校 ，团体，献血，服务，志愿者，等]
    窗口大小设置为5,
    [世界，献血，日，学校，团体]
    [献血，日，学校，团体，献血]
    [日，学校，团体...]
    每个窗口内的词都有连接关系，比如世界和献血，日，学校，团体。有个连接关系根据公式计算得分，得分最高的n个词作为文旦的关键词。

