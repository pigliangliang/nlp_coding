本系列为nlp相关原理和实践，语言为python
1、中文分词技术
  参考word_segmentation.py文件，包含基于规则分词分词方法，统计分词方法，以及py示例代码。
  结巴分词示例
  1 规则分词
   1）正向最大分词
        基本思想是：假定分词词典中最长词有一个汉字字符，则用被处理文档的当前字串的前i个字作为匹配字段，查找
        字典，若字典中存在这样的一个长度为i的字词，则匹配成功，匹配字段被做为一个词切分出来。如果字典中
        没有找到这个词，则匹配失败，将词的最后一个字去掉，对剩下的字串重新进行匹配处理。
        如此进行，知道匹配成功，即切出一个词。

   2）逆向最大匹配
        思想与正向相反。
   3）双向最大匹配
        思想：将正向与反向最大分词的结果进行比较，按照最大匹配原则，选取词数切分最少的最为结果。

  2、统计分词
  主要思想是：
  把每个词看作是由词的最小单位的各个字组成的，如果相连的字在不同的文本中出现次数越多，就证明这相连的字就很可能就是一个词。
  因此我们就可以利用字与字相邻出现的频率来反应词的可靠性，统计语料中相邻共现的各个字组合的频率，当这个频率高于某个临界值，
  就可以组成一个词。
  基于统计的分词：
  1）建立统计语言模型
  2）对句子进行单词划分，然后对分词结果进行概率计算，获得概率最大的分词结果。比如：HMM 和CRF

  语言模型：
  用概率论的专业术解释语言模型就是：为长度为m的字符串确定其概率分布。p(w1,w2.....wm),其中w1...wm依次表示文本中的各个词语。一般用链式法则计
  算概率值 p(w1,w2....)= P(w1)P(w2|w1)P(w3|w1,w2)....P(wm|w1,w2....wm-1)
  上式当文本过长，计算难度越来越大。
  n——gram模型：
  n=1:p(w1,w2....)=p(w1)p(w2)...p(wm)
  n=2:p(w1,w2....)=p(w1)p(w2|pw1)...p(wi|wi-1)
  n=2:p(w1,w2....)=p(w1)p(w2|pw1)...p(wi|wi-1,wi-2)

  HMM模型：
  隐马尔科夫模型将分词作为字在字符串中序列标注任务来实现的。基本思路是：每个字在构造一个特定的词语时都占据着一个确定的构词位置，规定每个字最多有
  四个构词位置：B词首，M词中，E词尾，S单独成词。
  用数学抽象表示：
  X=X1X2...Xn代表输入的句子，n表示句子的长度，Xn表示句中的字，Y=Y1Y2...Yn，为BMSE四中标记
  这里引入观测独立性行假设：即每个字段输出仅仅与当前字有关
  于是能得到公式：p(y1y2...yn|x1x2...xn) = p(y1|x1).....p(yn|xn)
  基于观测独立性假设，完全没有考虑上下文，计算容易，但是会出现不合理情况。
  引入HMM解决这个问题。
  上式中解决p（y|x）问题，根据贝叶斯公式，p（Y|X）= p(x|y)p(y)/p（x）
  对p（x|y）做马尔科夫驾驶
  p（x|y）=p(x1|y1)p(x2|y2)...p(xn|yn)
  对p(y)引入马尔科夫另一假设，每个输出仅仅个上一个输出有关系。
  p（y）= p(y1)p(y2|y1)...p(yn|yn-1)
  求解HMM的过程中使用了vetervi算法：核心思想是如果最终的最优路径经过某个xi，那么从最初节点到xi-1节点的路径也是最优路径。
2、词性标注和命名实体识别
  原理知识
  CRF的命名实体识别
  地名识别
  CRF：设X =(X1,X2...Xn) 和Y=(Y1,Y2...Yn)是联合随机变量，若随机变量y构成一个无向图表示的马尔科夫，则其条件概率P(Y|X)称为条件随机场。
  CRF++实现中文

